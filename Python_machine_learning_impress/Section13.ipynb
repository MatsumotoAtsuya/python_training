{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor flow >> GIL (global inter prilock)\n",
    "#Low layer API >> Layers, Keras\n",
    "import tensorflow as tf\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None), name=\"x\")\n",
    "    w = tf.Variable(2.0, name=\"weight\")\n",
    "    b = tf.Variable(0.7, name=\"bias\")\n",
    "    z = w * x + b\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(init)\n",
    "    for t in [1.0, 0.6, -1.8]:\n",
    "        print(\"x={:>4.1f} --> z={:>4.1f}\".format(t, sess.run(z, feed_dict={x:t})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place holder >> variables\n",
    "#feed_dict >> feeding(=provideing) dictionary\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(z, feed_dict={x: [1.0, 2.0, 3.0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None, 2, 3), name=\"input_x\")\n",
    "    x2 = tf.reshape(x, shape=(-1, 6), name=\"x2\")\n",
    "    \n",
    "    xsum = tf.reduce_sum(x2, axis=0, name=\"col_sum\")\n",
    "    xmean = tf.reduce_mean(x2, axis=0, name=\"col_mean\")\n",
    "with tf.Session(graph=g) as sess:\n",
    "    x_array = np.arange(18).reshape(3, 2, 3)\n",
    "    print(\"input shape:\", x_array.shape)\n",
    "    print(\"Reshaped:\\n\", sess.run(x2, feed_dict={x: x_array}) )\n",
    "    print(\"Column_sum:\\n\", sess.run(xsum, feed_dict={x: x_array}))\n",
    "    print(\"Column_means:\\n\", sess.run(xmean, feed_dict={x: x_array}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.arange(10).reshape((10, 1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0])\n",
    "\n",
    "class TfLinreg(object):\n",
    "    \n",
    "    def __init__(self, x_dim, learning_rate=0.01, random_seed=None):\n",
    "        self.x_dim = x_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.g = tf.Graph()\n",
    "        \n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(random_seed)\n",
    "            #building tensor flow\n",
    "            self.build()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self):\n",
    "        self.X = tf.placeholder(dtype=tf.float32, shape=(None, self.x_dim),\n",
    "                               name=\"x_input\")\n",
    "        self.y = tf.placeholder(dtype=tf.float32, shape=(None),\n",
    "                               name=\"y_input\")\n",
    "        print(self.X)\n",
    "        print(self.y)\n",
    "        #weight matrics and bias vector\n",
    "        w = tf.Variable(tf.zeros(shape=(1)), name=\"weight\")\n",
    "        b = tf.Variable(tf.zeros(shape=(1)), name=\"bias\")\n",
    "        print(w)\n",
    "        print(b)\n",
    "\n",
    "        self.z_net = tf.squeeze(w * self.X + b, name=\"z_net\")\n",
    "        print(self.z_net)\n",
    "\n",
    "        sqr_errors = tf.square(self.y - self.z_net, name=\"sqr_errors\")\n",
    "        print(sqr_errors)\n",
    "        self.mean_cost = tf.reduce_mean(sqr_errors, name=\"mean_cost\")\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "                    learning_rate=self.learning_rate,\n",
    "                    name=\"GradientDecent\")\n",
    "        self.optimizer = optimizer.minimize(self.mean_cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodel = TfLinreg(x_dim=X_train.shape[1], learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linreg(sess, model, X_train, y_train, num_epochs=10):\n",
    "    sess.run(model.init_op)\n",
    "    #initialize variables >> w, b\n",
    "    training_costs = []\n",
    "    for i in range(num_epochs):\n",
    "        n, cost = sess.run([model.optimizer, model.mean_cost],\n",
    "                          feed_dict={model.X:X_train, model.y:y_train})\n",
    "        print(n)\n",
    "        training_costs.append(cost)\n",
    "    return training_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=lrmodel.g)\n",
    "training_costs = train_linreg(sess, lrmodel, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(1, len(training_costs)+1), training_costs)\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training_cost\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linreg(sess, model, X_test):\n",
    "    y_pred = sess.run(model.z_net, feed_dict={model.X:X_test})\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, marker=\"s\", s=50, label=\"training_data\")\n",
    "plt.plot(range(X_train.shape[0]), predict_linreg(sess, lrmodel, X_train),\n",
    "        color=\"grey\", marker=\"o\", markersize=6, linewidth=3, label =\"LinReg Model\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers api , karas api\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "def load_mnist(path, kind=\"train\"):\n",
    "    labels_path = os.path.join(path, \"{}-labels.idx1-ubyte\".format(kind))\n",
    "    images_path = os.path.join(path, \"{}-images.idx3-ubyte\".format(kind))\n",
    "    \n",
    "    with open(labels_path, \"rb\") as lbpath:\n",
    "        magic, n = struct.unpack(\">II\", lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "        \n",
    "    with open(images_path, \"rb\") as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.0) - 0.5) ** 2\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist(\".\", kind=\"train\")\n",
    "print(\"Rows: {:d}, Columns: {:d}\".format(X_train.shape[0], X_train.shape[1]))\n",
    "X_test, y_test = load_mnist(\".\", kind=\"t10k\")\n",
    "print(\"Rows: {:d}, Columns: {:d}\".format(X_test.shape[0], X_test.shape[1]))\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "X_train_centered = (X_train - mean_vals) / std_val\n",
    "X_test_centered = (X_test - mean_vals) / std_val\n",
    "del X_train, X_test\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "print(X_test_centered.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "n_features = X_train_centered.shape[1]\n",
    "n_classes = 10\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    tf_x = tf.placeholder(dtype=tf.float32, shape=(None, n_features), name=\"tf_x\")\n",
    "    tf_y = tf.placeholder(dtype=tf.int32, shape=None, name=\"tf_y\")\n",
    "    y_onehot = tf.one_hot(indices=tf_y,depth=n_classes)\n",
    "    h1 = tf.layers.dense(inputs=tf_x, units=50, activation=tf.tanh, name=\"layer1\")\n",
    "    h2 = tf.layers.dense(inputs=h1, units=50, activation=tf.tanh, name=\"layer2\")\n",
    "    logits = tf.layers.dense(inputs=h2, units=10, activation=None, name=\"layer3\")\n",
    "    predictions = {\n",
    "        \"classes\" : tf.argmax(logits, axis=1, name=\"predicted_classes\"),\n",
    "        \"probablities\" : tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    cost = tf.losses.softmax_cross_entropy(onehot_labels=y_onehot, logits=logits)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optimizer.minimize(loss=cost)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(X, y, batch_size=128, shuffle=False):\n",
    "    X_copy = np.array(X)\n",
    "    y_copy = np.array(y)\n",
    "    if shuffle:\n",
    "        data = np.column_stack((X_copy, y_copy))\n",
    "        np.random.shuffle(data)\n",
    "        X_copy = data[:, :-1]\n",
    "        y_copy = data[:, -1].astype(int)\n",
    "        \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X_copy[i: i+batch_size, :], y_copy[i: i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=g)\n",
    "sess.run(init_op)\n",
    "#50 epochs in training\n",
    "training_costs = []\n",
    "for epoch in range(50):\n",
    "    training_loss = []\n",
    "    batch_generator = create_batch_generator(X_train_centered, y_train, batch_size=64)\n",
    "    for batch_X, batch_y, in batch_generator:\n",
    "        feed = {tf_x: batch_X, tf_y: batch_y}\n",
    "        _, batch_cost = sess.run([train_op, cost], feed_dict=feed)\n",
    "        training_costs.append(batch_cost)\n",
    "    print(\" -- Epoch {:2d} AVG. training loss : {:.4f}\".format(epoch+1, np.mean(training_costs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed =  {tf_x : X_test_centered}\n",
    "y_pred = sess.run(predictions[\"classes\"], feed_dict=feed)\n",
    "print(\"Test Accuracy: {:.2f}\".format(100 * np.sum(y_pred == y_test) / y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use Karas!\n",
    "#instead of code below, you can use it.\n",
    "#from kares.datasets import mnist\n",
    "#(train_imges, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "X_train,  y_train = load_mnist(\"./\", kind=\"train\")\n",
    "print(\"Rows : {:d}, Cols : {:d}\".format(X_train.shape[0], X_train.shape[1]))\n",
    "X_test, y_test = load_mnist(\"./\", kind=\"t10k\")\n",
    "print(\"Rows : {:d}, Cols : {:d}\".format(X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "#Standardization & Reguralization\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "print(mean_vals.shape)\n",
    "std_val = np.std(X_train)\n",
    "print(std_val.shape, std_val)\n",
    "X_train_centered = (X_train - mean_vals) / std_val\n",
    "X_test_centered = (X_test - mean_vals) / std_val\n",
    "\n",
    "del X_train, X_test\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "np.random.seed(123)\n",
    "tf.set_random_seed(123)\n",
    "y_train_onehot = keras.utils.to_categorical(y_train)\n",
    "print(\"First 3 labels:\", y_train[:3])\n",
    "print(\"\\n First 3 labels in onehote: \\n\", y_train_onehot[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets implementation neural network\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(units=50, input_dim=X_train_centered.shape[1],\n",
    "                            kernel_initializer=\"glorot_uniform\",\n",
    "                             bias_initializer=\"zeros\", activation=\"tanh\"))\n",
    "model.add(keras.layers.Dense(units=50, input_dim=50, \n",
    "                            kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\", activation=\"tanh\"))\n",
    "model.add(keras.layers.Dense(units=y_train_onehot.shape[1], input_dim=50,\n",
    "                            kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\", activation=\"softmax\"))\n",
    "#model compiling\n",
    "sgd_optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-7, momentum=0.9)\n",
    "model.compile(optimizer=sgd_optimizer, loss=\"categorical_crossentropy\")\n",
    "#Glorot initialization >> famous method\n",
    "#keras.optimizer.SGD class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_centered, y_train_onehot, batch_size=64,\n",
    "                   epochs=50, verbose=1, validation_split=0.1)\n",
    "#verbose >> output comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "print(\"first 3 predicttions :\",y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "correct_preds = np.sum(y_train == y_train_pred, axis=0)\n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "print(\"First 3 predictions:\", y_train_pred[:3])\n",
    "print(\"Training accuracy: {:.2f}\".format(train_acc * 100))\n",
    "y_test_pred = model.predict_classes(X_test_centered, verbose=0)\n",
    "correct_preds = np.sum(y_test == y_test_pred, axis=0)\n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "print(\"First 3 predictions:\", y_test_pred[:3])\n",
    "print(\"Test accuracy: {:.2f}\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic function takes much time, so hyperbolic tangent function is very useful\n",
    "import numpy as np \n",
    "X = np.array([1, 1.4, 2.5])\n",
    "w = np.array([0.4, 0.3, 0.5])\n",
    "def net_input(X, w):\n",
    "    return np.dot(X, w)\n",
    "\n",
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return logistic(z)\n",
    "\n",
    "print(\"P(y=1|x) = {:.3f}\".format(logistic_activation(X, w)))\n",
    "#Possibility of class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1.1, 1.2, 0.8, 0.4],\n",
    "              [0.2, 0.5, 1.0, 0.3],\n",
    "              [0.4, 1.1, 0.5, 0.8]])\n",
    "#A[0][0] must be 1.0\n",
    "A = np.array([[1.0, 0.1, 1.5, 0.5]])\n",
    "\n",
    "Z = np.dot(W, A[0])\n",
    "y_probables = logistic(Z)\n",
    "print(\"Net Input:\", Z)\n",
    "print(\"OUT put units\", y_probables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class = np.argmax(Z, axis=0)\n",
    "print(\"Predicted Class Label: {:d}\".format(y_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "y_probas = softmax(Z)\n",
    "print(\"Probabilities:\", y_probas)\n",
    "np.sum(y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_hyperbolic tangent_\n",
    "$(exp(x) - exp(-x)) / (exp(x) + (exp(-x)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def tanh(z):\n",
    "    e_p = np.exp(z)\n",
    "    e_n = np.exp(-z)\n",
    "    return (e_p - e_n) / (e_p + e_n)\n",
    "\n",
    "z = np.arange(-5, 5, 0.0005)\n",
    "logistic_act = logistic(z)\n",
    "tanh_act = tanh(z)\n",
    "plt.ylim = ([-1.5, 1.5])\n",
    "plt.xlabel(\"Net input $z$\")\n",
    "plt.ylabel(\"activation $/phi(z)$\")\n",
    "for n in [1, 0.5, 0, -0.5, -1]:\n",
    "    plt.axhline(n, color=\"black\", linestyle=\":\")\n",
    "plt.plot(z, tanh_act, linewidth=3, linestyle=\"--\", label=\"tanh\")\n",
    "plt.plot(z, logistic_act, linewidth=3, label=\"logistic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_act = np.tanh(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "log_act = expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
