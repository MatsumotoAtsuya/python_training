{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "### Sequence >> not IID(Independent and Identically Distributed)\n",
    "### Backpropagation Through Time >> One of RNN model\n",
    "#### problems in BPTT\n",
    "- vanishing gradient $|w_{hh}| < 1$\n",
    "- exploding gradient $|w_{hh}| > 1$\n",
    "#### Solutions in BPTT\n",
    "- T-BPTT(Truncated Backpropagation Through Time)\n",
    "- LSTM (Long and Short Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNITS in LSTM\n",
    "- memory cell >> that has cell state and three gates\n",
    "    - forget gate >> only sigmoid\n",
    "    - input gate (& input node ) >> tanh and sigmoid\n",
    "    - output gate >> tanh and sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind \n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"movie_data.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't use \"set\" in finding unique word, instead use Counter \n",
    "from collections import Counter\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df[\"review\"]), title=\"counting words occurrences\")\n",
    "for i, review in enumerate(df[\"review\"]):\n",
    "    text = \"\".join([c if c not in punctuation else \" \"+c+\" \" for c in review]).lower()\n",
    "    df.loc[i, \"review\"] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "#pyprind.ProgBar >> tracking progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "#enumerate(iter, start, end)\n",
    "mapped_reviews = []\n",
    "pber = pyprind.ProgBar(len(df[\"review\"]), title=\"map reviews to ints\")\n",
    "for review in df[\"review\"]:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, \"sentiment\"].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, \"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function >> mini batch\n",
    "np.random.seed(123)\n",
    "\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x) // batch_size\n",
    "    x = x[:n_batches * batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii: ii+batch_size], y[ii: ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii: ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_words = len(word_counts)\n",
    "# embedding_size = 200\n",
    "# embedding = tf.Variable(\n",
    "# tf.random_uniform(shape=(n_words, embedding_size), minval=-1, maxval=1))\n",
    "# #call tf.nn.embedding_lookup function\n",
    "# embed_x = tf.nn.embedding_lookup(embedding, tf_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructiong RNN model\n",
    "- constructor\n",
    "- build method >> input_data, input_label, keep_dropout\n",
    "- train method >> training\n",
    "- predict methd >> new seesion and prediction new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_length=200, lstm_size=256, num_layers=1, \n",
    "                batch_size=64, learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_length = seq_length\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "\n",
    "    def build(self):\n",
    "        tf_x = tf.placeholder(tf.int32, shape=(self.batch_size, self.seq_length), name=\"tf_x\")\n",
    "        tf_y = tf.placeholder(tf.float32, shape=(self.batch_size), name=\"tf_y\")\n",
    "        tf_keep_prob = tf.placeholder(tf.float32, name=\"tf_keep_prob\")\n",
    "        embedding = tf.Variable(tf.random_uniform((self.n_words, self.embed_size),\n",
    "                                                 minval=-1, maxval=1), name=\"embedding\")\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name=\"embed_x\")\n",
    "        #lstm cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "        [tf.contrib.rnn.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "        output_keep_prob = tf_keep_prob)\n",
    "        for _ in range(self.num_layers)])\n",
    "\n",
    "        #Define initial State\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print(\" << initial State >> \", self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x, \n",
    "                                                           initial_state = self.initial_state)\n",
    "        #lstm_outputs = [batch_size, max_time, cells.output_size]\n",
    "        print(\"\\n << lstm output >> \", lstm_outputs)\n",
    "        print(\"\\n << final state >> \", self.final_state)\n",
    "\n",
    "        #full connected layer\n",
    "        logits = tf.layers.dense(inputs=lstm_outputs[: ,-1],\n",
    "                                units=1, activation=None, name=\"logits\")\n",
    "        logits = tf.squeeze(logits, name=\"logits_squeezed\")\n",
    "        #tf.squeeze >> Removes dimensions of size 1 from the shape of a tensor\n",
    "        print(\"\\n << logits      >> \", logits)\n",
    "        y_proba = tf.nn.sigmoid(logits, name=\"probabilities\")\n",
    "        predictions = {\n",
    "            \"probabilities\": y_proba,\n",
    "            \"labels\": tf.cast(tf.round(y_proba), tf.int32, name=\"labels\")\n",
    "        }\n",
    "        print(\"\\n << predictions >> \", predictions)\n",
    "        cost = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=logits), name=\"cost\")\n",
    "\n",
    "        #Define Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name=\"train_op\")\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "\n",
    "                if epoch == 0:\n",
    "                    self.saver.save(sess, \"model/sentiment-{:d}.ckpt\".format(epoch))\n",
    "                    \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                                            X_train, y_train, self.batch_size):\n",
    "                    feed = {\"tf_x:0\": batch_x, \"tf_y:0\": batch_y,\n",
    "                           \"tf_keep_prob:0\": 0.5, self.initial_state: state}\n",
    "                    loss, _, state = sess.run([\n",
    "                        \"cost:0\", \"train_op\", self.final_state\n",
    "                    ], feed_dict = feed)\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: {:d}/{:d}  Iteration: {:d} | Training loss: {:.5f}\".format(\n",
    "                        epoch + 1, num_epochs, iteration, loss))\n",
    "                    iteration +=1\n",
    "                    \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    self.saver.save(sess, \"model/sentiment-{:d}.ckpt\".format(epoch))\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint(\".\\model\\\\\"))\n",
    "        test_state = sess.run(self.initial_state)\n",
    "        for ii, batch_x in enumerate(create_batch_generator(\n",
    "                        X_data, None, batch_size=self.batch_size), 1):\n",
    "            feed = {\"tf_x:0\": batch_x, \"tf_keep_prob:0\": 1.0,\n",
    "                   self.initial_state: test_state}\n",
    "            if return_proba:\n",
    "                pred, test_state = sess.run([\"probabilities:0\", self.final_state],\n",
    "                                           feed_dict=feed)\n",
    "            else:\n",
    "                pred, test_state = sess.run([\"labels:0\", self.final_state],\n",
    "                                           feed_dict=feed)\n",
    "\n",
    "            preds.append(pred)\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words, seq_length=sequence_length, embed_size=256,\n",
    "                  lstm_size=128, num_layers=1, batch_size=100, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print(\"Test Accurcy: {:.3f}\".format(np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = rnn.predict(X_test, return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Language model of RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
