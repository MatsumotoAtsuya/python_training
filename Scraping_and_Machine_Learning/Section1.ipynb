{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "save_name = \"test.png\"\n",
    "urllib.request.urlretrieve(url, save_name)\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "save_name = \"test1.png\"\n",
    "#downloding\n",
    "memory = urllib.request.urlopen(url).read()\n",
    "with open(save_name, mode=\"wb\") as f:\n",
    "    f.write(memory)\n",
    "    print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"http://api.aoikujira.com/ip/ini\"\n",
    "res = urllib.request.urlopen(url)\n",
    "data = res.read()\n",
    "text = data.decode(\"utf-8\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "API = \"http://api.aoikujira.com/zip/xml/get.php\"\n",
    "values = {\n",
    "    \"fmt\": \"xlm\", #fmt >> format\n",
    "    \"zn\": \"5600045\"\n",
    "}\n",
    "params = urllib.parse.urlencode(values)\n",
    "url = API + \"?\" + params\n",
    "print(\"url :\",url)\n",
    "data = urllib.request.urlopen(url).read()\n",
    "text = data.decode(\"utf-8\")\n",
    "print(data, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request as request\n",
    "import urllib.parse as parse\n",
    "#if you use python shell, use codes blelow\n",
    "# if len(sys.argv) <= 1:\n",
    "#     print(\"usage:\" , hyakunin.py(keyword))\n",
    "#     keyword = sys.argv[1]\n",
    "#####################################\n",
    "\n",
    "#if you use jupyter, use codes below\n",
    "keyword = input()\n",
    "##########################\n",
    "print(keyword)\n",
    "API = \"http://api.aoikujira.com/hyakunin/get.php\"\n",
    "query = {\n",
    "    \"fmt\": \"ini\",\n",
    "    \"key\": keyword\n",
    "}\n",
    "param = parse.urlencode(query)\n",
    "url = API + \"?\" + param\n",
    "print(\"url: \",url)\n",
    "\n",
    "with request.urlopen(url) as f:\n",
    "    bi = f.read()\n",
    "    data = bi.decode(\"utf-8\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "html='''\n",
    "<html>\n",
    "<body>\n",
    "<h1>What's is scraping?</h1>\n",
    "<p>Analize HTML page</p>\n",
    "<p>Extraction Elements</p>\n",
    "</body>\n",
    "</html>'''\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "print(soup)\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "print(\"h1:\", h1.string)\n",
    "print(\"p1:\", p1.string)\n",
    "print(\"p2:\", p2.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html='''\n",
    "<html>\n",
    "<body>\n",
    "<h1 id=\"title\">What's is scraping?</h1>\n",
    "<p id=\"txt\">Analize HTML page</p>\n",
    "<p>Extraction Elements</p>\n",
    "</body>\n",
    "</html>'''\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "title = soup.find(id=\"title\")\n",
    "txt  = soup.find(id=\"txt\")\n",
    "print(\"#title: \", title.string)\n",
    "print(\"#txt: \", txt.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = '''\n",
    "<html><body>\n",
    "<ul>\n",
    "<li><a href=\"http://uta.pw\">uta</a></li>\n",
    "<li><a href=\"http://oto.chu.jp\">oto</a></li>\n",
    "</ul>\n",
    "</body></html>'''\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "links = soup.find_all(\"a\")\n",
    "for a in links:\n",
    "    href = a.attrs[\"href\"]\n",
    "    text = a.string\n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())\n",
    "print(a.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as request\n",
    "\n",
    "url = \"http://api.aoikujira.com/zip/xml/3300075\"\n",
    "res = request.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "ken = soup.find(\"ken\").string\n",
    "shi = soup.find(\"shi\").string\n",
    "cho = soup.find(\"cho\").string\n",
    "print(\"ken, shi, cho >>\", ken, shi, cho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = '''\n",
    "<html><body>\n",
    "<div id=\"works\">\n",
    "<h1>紅玉いづきの作品リスト</h1>\n",
    "<ul class=\"items\">\n",
    "<li>ミミズクと夜の王</li>\n",
    "<li>MAMA</li>\n",
    "<li>雪蟷螂</li>\n",
    "</ul>\n",
    "</div>\n",
    "</body></html>'''\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "h1 = soup.select_one(\"div#works > h1\").string\n",
    "print(\"h1: \",h1)\n",
    "li_list = soup.select(\"div#works > ul.items > li\")\n",
    "print(li_list)\n",
    "for li in li_list:\n",
    "    print(\"li: \", li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as request\n",
    "\n",
    "url = \"http://stocks.finance.yahoo.co.jp/stocks/detail/?code=usdjpy\"\n",
    "response = request.urlopen(url)\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "price = soup.select_one(\".stoksPrice\").string\n",
    "print(\"usd/jpy: \", price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as request\n",
    "url = \"http://api.aoikujira.com/kawase/xml/usd\"\n",
    "response = request.urlopen(url)\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "print(soup)\n",
    "jpy = soup.select_one(\"jpy\").string \n",
    "print(\"usd/jpy: \", jpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as request\n",
    "\n",
    "url = \"http://www.aozora.gr.jp/index_pages/person148.html\"\n",
    "response = request.urlopen(url)\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "li_list = soup.select(\"ol > li\")\n",
    "for li in li_list:\n",
    "    a = li.a\n",
    "    if a != None:\n",
    "        name = a.string\n",
    "        href = a.attrs[\"href\"]\n",
    "        print(name, \">\", href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# fp = open(\"books.html\", encoding=\"utf-8\")\n",
    "# soup = BeautifulSoup(fp, \"html.parser\")\n",
    "html = '''\n",
    "<html><body>\n",
    "<ul id=\"bible\">\n",
    "<li id=\"num\">Numbers</li>\n",
    "</ul>\n",
    "</html></body>'''\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "sel = lambda q: print(soup.select_one(q).string)\n",
    "sel(\"#num\")\n",
    "sel(\"li#num\")\n",
    "sel(\"ul > li#num\")\n",
    "sel(\"#bible > #num\")\n",
    "sel(\"ul#bible > li#num\")\n",
    "sel(\"ul#bible > #num\")\n",
    "sel(\"li[id='num']\")\n",
    "sel(\"li:nth-of-type(1)\")\n",
    "print(soup.select(\"li\")[0].string)\n",
    "print(soup.find_all(\"li\")[0].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "fp = open(\"fruits-vegetables.html\", encoding=\"utf-8\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "print(soup.select_one(\"li:nth-of-type(8)\").string)\n",
    "print(soup.select_one(\"#ve-list > li:nth-of-type(4)\").string)\n",
    "print(soup.select(\"#ve-list > li[data-lo='us']\")[1].string)\n",
    "print(soup.select(\"#ve-list > li.black\")[1].string)\n",
    "condition = {\"data-lo\": \"us\", \"class\": \"black\"}\n",
    "print(soup.find(\"li\", condition).string)\n",
    "print(soup.find(id=\"ve-list\").find(\"li\", condition).string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html = '''<ul>\n",
    "<li><a href=\"first.html\">first</li>\n",
    "<li><a href=\"https://example.com/second\">second</li>\n",
    "<li><a href=\"https://example.com/third\">third</li>\n",
    "<li><a href=\"http://example.com\"/forth>forth</li>\n",
    "</ul>'''\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "li = soup.find_all(href=re.compile(r\"^https://\"))\n",
    "print(li)\n",
    "for e in li: print(e.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "base = \"http://example.com/html/a.html\"\n",
    "print(urljoin(base, \"b.html\"))\n",
    "print(urljoin(base, \"sub/c.html\"))\n",
    "print(urljoin(base, \"../index.html\"))\n",
    "print(urljoin(base, \"../img/hoge.html\"))\n",
    "print(urljoin(base, \"../css/hoge.css\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "base = \"http://example.com/html/a.html\"\n",
    "print(urljoin(base, \"/hoge.html\"))\n",
    "print(urljoin(base, \"http://kaufen.com/wiki\"))\n",
    "print(urljoin(base, \"//ute.pw/shodow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request \n",
    "import urllib.parse\n",
    "import  time, re\n",
    "import os\n",
    "proc_files = {}\n",
    "\n",
    "#Extract Files\n",
    "def enum_links(html, base):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.select(\"link[rel='stylesheet']\")\n",
    "    links += soup.select(\"a[href]\")\n",
    "    result = []\n",
    "    for a in links:\n",
    "        href = a.attrs[\"href\"]\n",
    "        print(href)\n",
    "        url = urllib.parse.urljoin(base, href)\n",
    "        result.append(url)\n",
    "    return result\n",
    "\n",
    "def download_files(url):\n",
    "    output = urllib.parse.urlparse(url)\n",
    "    savepath = \"./\" + output.netloc + output.path\n",
    "    if re.search(r\"/$\", savepath):\n",
    "        savepath += \"index.html\"\n",
    "    savedir = os.path.dirname(savepath)\n",
    "    \n",
    "    #download files\n",
    "    if not os.path.exists(savedir):\n",
    "        print(\"mkdir:\", savedir)\n",
    "        os.makedirs(savedir)\n",
    "    try:\n",
    "        print(\"download: \", url)\n",
    "        urllib.request.urlretrieve(url, savepath)\n",
    "        time.sleep(1)\n",
    "        return savepath\n",
    "    except:\n",
    "        print(\"Failure to download: \", url)\n",
    "        return None\n",
    "\n",
    "#HTML analyzer and downloader\n",
    "def analize_html(url, root_url):\n",
    "    savepath = download_files(url)\n",
    "    if savepath is None:\n",
    "        return print(\"savepath doesn't exist\")\n",
    "    if savepath in proc_files:\n",
    "        return print(\"already analized\")\n",
    "    proc_files[savepath] = True\n",
    "    print(\"analyze html:\", savepath)\n",
    "    html = open(savepath, \"r\", encoding=\"utf-8\").read()\n",
    "    links = enum_links(html, url)\n",
    "    for link_url in links:\n",
    "        if link_url.find(root_url) != 0:\n",
    "            if not re.search(r\".css$\", link_url):\n",
    "                continue\n",
    "            if re.search(r\".(html|htm)$\", link_url):\n",
    "                analize_html(link_url, root_url)\n",
    "            download_files(link_url)\n",
    "            \n",
    "url = \"http://docs.python.jp/3.6/library/\"\n",
    "analize_html(url, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
