{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "### Sequence >> not IID(Independent and Identically Distributed)\n",
    "### Backpropagation Through Time >> One of RNN model\n",
    "#### problems in BPTT\n",
    "- vanishing gradient $|w_{hh}| < 1$\n",
    "- exploding gradient $|w_{hh}| > 1$\n",
    "#### Solutions in BPTT\n",
    "- T-BPTT(Truncated Backpropagation Through Time)\n",
    "- LSTM (Long and Short Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNITS in LSTM\n",
    "- memory cell >> that has cell state and three gates\n",
    "    - forget gate >> only sigmoid\n",
    "    - input gate (& input node ) >> tanh and sigmoid\n",
    "    - output gate >> tanh and sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind \n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"movie_data.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't use \"set\" in finding unique word, instead use Counter \n",
    "from collections import Counter\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df[\"review\"]), title=\"counting words occurrences\")\n",
    "for i, review in enumerate(df[\"review\"]):\n",
    "    text = \"\".join([c if c not in punctuation else \" \"+c+\" \" for c in review]).lower()\n",
    "    df.loc[i, \"review\"] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "#pyprind.ProgBar >> tracking progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "#enumerate(iter, start, end)\n",
    "mapped_reviews = []\n",
    "pber = pyprind.ProgBar(len(df[\"review\"]), title=\"map reviews to ints\")\n",
    "for review in df[\"review\"]:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, \"sentiment\"].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, \"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function >> mini batch\n",
    "np.random.seed(123)\n",
    "\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x) // batch_size\n",
    "    x = x[:n_batches * batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii: ii+batch_size], y[ii: ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii: ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_words = len(word_counts)\n",
    "# embedding_size = 200\n",
    "# embedding = tf.Variable(\n",
    "# tf.random_uniform(shape=(n_words, embedding_size), minval=-1, maxval=1))\n",
    "# #call tf.nn.embedding_lookup function\n",
    "# embed_x = tf.nn.embedding_lookup(embedding, tf_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructiong RNN model\n",
    "- constructor\n",
    "- build method >> input_data, input_label, keep_dropout\n",
    "- train method >> training\n",
    "- predict methd >> new seesion and prediction new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_length=200, lstm_size=256, num_layers=1, \n",
    "                batch_size=64, learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_length = seq_length\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "\n",
    "    def build(self):\n",
    "        tf_x = tf.placeholder(tf.int32, shape=(self.batch_size, self.seq_length), name=\"tf_x\")\n",
    "        tf_y = tf.placeholder(tf.float32, shape=(self.batch_size), name=\"tf_y\")\n",
    "        tf_keep_prob = tf.placeholder(tf.float32, name=\"tf_keep_prob\")\n",
    "        embedding = tf.Variable(tf.random_uniform((self.n_words, self.embed_size),\n",
    "                                                 minval=-1, maxval=1), name=\"embedding\")\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name=\"embed_x\")\n",
    "        #lstm cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "        [tf.contrib.rnn.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "        output_keep_prob = tf_keep_prob)\n",
    "        for _ in range(self.num_layers)])\n",
    "\n",
    "        #Define initial State\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print(\" << initial State >> \", self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x, \n",
    "                                                           initial_state = self.initial_state)\n",
    "        #lstm_outputs = [batch_size, max_time, cells.output_size]\n",
    "        print(\"\\n << lstm output >> \", lstm_outputs)\n",
    "        print(\"\\n << final state >> \", self.final_state)\n",
    "\n",
    "        #full connected layer\n",
    "        logits = tf.layers.dense(inputs=lstm_outputs[: ,-1],\n",
    "                                units=1, activation=None, name=\"logits\")\n",
    "        logits = tf.squeeze(logits, name=\"logits_squeezed\")\n",
    "        #tf.squeeze >> Removes dimensions of size 1 from the shape of a tensor\n",
    "        print(\"\\n << logits      >> \", logits)\n",
    "        y_proba = tf.nn.sigmoid(logits, name=\"probabilities\")\n",
    "        predictions = {\n",
    "            \"probabilities\": y_proba,\n",
    "            \"labels\": tf.cast(tf.round(y_proba), tf.int32, name=\"labels\")\n",
    "        }\n",
    "        print(\"\\n << predictions >> \", predictions)\n",
    "        cost = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=logits), name=\"cost\")\n",
    "\n",
    "        #Define Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name=\"train_op\")\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "\n",
    "                if epoch == 0:\n",
    "                    self.saver.save(sess, \"model/sentiment-{:d}.ckpt\".format(epoch))\n",
    "                    \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                                            X_train, y_train, self.batch_size):\n",
    "                    feed = {\"tf_x:0\": batch_x, \"tf_y:0\": batch_y,\n",
    "                           \"tf_keep_prob:0\": 0.5, self.initial_state: state}\n",
    "                    loss, _, state = sess.run([\n",
    "                        \"cost:0\", \"train_op\", self.final_state\n",
    "                    ], feed_dict = feed)\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: {:d}/{:d}  Iteration: {:d} | Training loss: {:.5f}\".format(\n",
    "                        epoch + 1, num_epochs, iteration, loss))\n",
    "                    iteration +=1\n",
    "                    \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    self.saver.save(sess, \"model/sentiment-{:d}.ckpt\".format(epoch))\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint(\".\\model\\\\\"))\n",
    "        test_state = sess.run(self.initial_state)\n",
    "        for ii, batch_x in enumerate(create_batch_generator(\n",
    "                        X_data, None, batch_size=self.batch_size), 1):\n",
    "            feed = {\"tf_x:0\": batch_x, \"tf_keep_prob:0\": 1.0,\n",
    "                   self.initial_state: test_state}\n",
    "            if return_proba:\n",
    "                pred, test_state = sess.run([\"probabilities:0\", self.final_state],\n",
    "                                           feed_dict=feed)\n",
    "            else:\n",
    "                pred, test_state = sess.run([\"labels:0\", self.final_state],\n",
    "                                           feed_dict=feed)\n",
    "\n",
    "            preds.append(pred)\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words, seq_length=sequence_length, embed_size=256,\n",
    "                  lstm_size=128, num_layers=1, batch_size=100, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print(\"Test Accurcy: {:.3f}\".format(np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = rnn.predict(X_test, return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Language model of RNN\n",
    "import numpy as np\n",
    "#readin text_data\n",
    "with open(\"2265.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[15858:]\n",
    "chars = set(text)\n",
    "char2int = {ch: i for i, ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    tot_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / tot_batch_length)\n",
    "    \n",
    "    if num_batches * tot_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches -1\n",
    "    \n",
    "    x = sequence[0: num_batches * tot_batch_length]\n",
    "    y = sequence[1: num_batches * tot_batch_length + 1]\n",
    "    #spliting batch\n",
    "    x_batch_split = np.split(x, batch_size)\n",
    "    y_batch_split = np.split(y, batch_size)\n",
    "    #connecting batch\n",
    "    x = np.stack(x_batch_split)\n",
    "    y = np.stack(y_batch_split)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape\n",
    "    num_batches = int(tot_batch_length / num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield (data_x[:, b*num_steps: (b+1)*num_steps],\n",
    "              data_y[:, b*num_steps: (b+1)*num_steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## character base RNN\n",
    "- constructor\n",
    "- build method\n",
    "- train method\n",
    "- sample method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=100,\n",
    "                lstm_size=128, num_layers=1, learning_rate=0.001, \n",
    "                keep_prob=0.5, grad_clip=5, sampling=False):\n",
    "    #gradient_clipping >> prevent divergence\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            print(\"saver\", self.saver)\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "\n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        tf_x = tf.placeholder(tf.int32, shape=[batch_size, num_steps], name=\"tf_x\")\n",
    "        tf_y = tf.placeholder(tf.int32, shape=[batch_size, num_steps], name=\"tf_y\")\n",
    "        tf_keepprob = tf.placeholder(tf.float32, name=\"tf_keepprob\")\n",
    "        #one_hot encoding\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "\n",
    "        #construct multiple RNN cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                output_keep_prob=tf_keepprob) for _ in range(self.num_layers)])\n",
    "        #initial state\n",
    "        self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "        #Implement each sequence step in RNN\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, x_onehot, initial_state=self.initial_state)\n",
    "        print( \" << letm outputs >>\" , lstm_outputs)\n",
    "        print(\" << final_state >> \", self.final_state)\n",
    "        seq_output_reshaped = tf.reshape(lstm_outputs, shape=[-1, self.lstm_size],\n",
    "                                        name=\"seq_output_reshaped\")\n",
    "        #Full connected layer\n",
    "        logits = tf.layers.dense(inputs=seq_output_reshaped, units=self.num_classes, \n",
    "                                 activation=None, name=\"logits\")\n",
    "        #probabilities of next batch\n",
    "        proba = tf.nn.softmax(logits, name=\"probabilities\")\n",
    "        y_reshaped = tf.reshape(y_onehot, shape=[-1, self.num_classes],\n",
    "                               name=\"y_reshaped\")\n",
    "        cost = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped), name=\"cost\")\n",
    "\n",
    "        #clipping gradients\n",
    "        train_vars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, train_vars), self.grad_clip)\n",
    "\n",
    "        #OPTIMIZER\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, train_vars), name=\"train_op\")\n",
    "\n",
    "    def train(self, train_x, train_y, num_epochs, ckpt_dir=\".\\model\\\\\"):\n",
    "        if not os.path.isdir(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            \n",
    "            n_batches = int(train_x.shape[1] / self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "            for epoch in range(num_epochs):\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "\n",
    "                batch_gen = create_batch_generator(train_x, train_y, self.num_steps)\n",
    "                for b, (batch_x, batch_y) in enumerate(batch_gen, 1):\n",
    "                    iteration = epoch * n_batches + b\n",
    "\n",
    "                    feed = {\"tf_x:0\": batch_x, \"tf_y:0\": batch_y, \"tf_keepprob:0\": self.keep_prob,\n",
    "                           self.initial_state: new_state}\n",
    "                    batch_cost, _, new_state = sess.run([\n",
    "                        \"cost:0\", \"train_op\", self.final_state\n",
    "                    ], feed_dict=feed)\n",
    "\n",
    "                    if iteration % 10 == 0:\n",
    "                        print(\"epoch {:d}/{:d}  iteration {:d} | Training loss : {:.4f}\".format(\n",
    "                                (epoch + 1), num_epochs, iteration, batch_cost))\n",
    "                self.saver.save(sess, os.path.join(ckpt_dir, \"language_modeling.ckpt\"))\n",
    "\n",
    "    def sample(self, output_length, ckpt_dir, starter_seq=\"The \"):\n",
    "        observed_seq = [ch for ch in starter_seq]\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
    "            #use starter_weq\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0, 0] = char2int[ch]\n",
    "                feed = {\"tf_x:0\": x, \"tf_keepprob:0\": 1.0, self.initial_state: new_state}\n",
    "                proba, new_state = sess.run([\"probabilities:0\", self.final_state],\n",
    "                                           feed_dict=feed)\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                #chars >> gloabal variable\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "            #model execution with observed_sep\n",
    "            for i in range(output_length):\n",
    "                x[0, 0] = ch_id\n",
    "                feed = {\"tf_x:0\": x, \"tf_keepprob:0\": 1.0,\n",
    "                       self.initial_state: new_state}    \n",
    "                proba, new_state = sess.run([\"probabilities:0\", self.final_state],\n",
    "                                           feed_dict=feed)\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "        return \"\".join(observed_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100\n",
    "train_x, train_y = reshape_data(text_ints, batch_size, num_steps)\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y, num_epochs=100, ckpt_dir=\".\\model-100\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rnn\n",
    "\n",
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "print(rnn.sample(ckpt_dir=\".\\model\\\\\", output_length=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
