{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"movie_data.csv\", encoding=\"utf-8\")\n",
    "df.loc[0, \"review\"][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub(\"<[^.]>\", \"\", text)\n",
    "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
    "    text = (re.sub(\"[\\W]+\", \" \", text.lower()) + \"\".join(emoticons).replace(\"-\", \"\"))\n",
    "    return text\n",
    "df[\"review\"] = df[\"review\"].apply(preprocessor)\n",
    "df.loc[0, \"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, \"review\"].values\n",
    "y_train = df.loc[:25000, \"sentiment\"].values\n",
    "X_test = df.loc[25000:, \"review\"].values\n",
    "y_test = df.loc[25000:, \"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "# tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None,\n",
    "#                        ngram_range=(1, 1), stop_words=None, tokenizer=tokenizer) \n",
    "# pipe_lr  = make_pipeline(tfidf, LogisticRegression(random_state=0, penalty=\"l2\", C=10.0))\n",
    "# pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error=\"igonre\", n_features=2**21, preprocessor=None,tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss=\"log\", random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(path=\"movie_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = tfidf.fit_transform(X_test)\n",
    "# clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make an application!!\n",
    "#model persistence >> シリアライズ\n",
    "import pickle\n",
    "import os\n",
    "desti = os.path.join(\"movieclassifier\", \"pkl_objects\")\n",
    "if not os.path.exists(desti):\n",
    "    os.makedirs(desti)\n",
    "    #protocol >> python 3.x x=4\n",
    "pickle.dump(stop, open(os.path.join(desti, \"stopwords.pkl\"), \"wb\"), protocol=4)\n",
    "#protocol >> python 3.x 以上のもので互換性を持たせる。この場合はx=4。\n",
    "pickle.dump(clf, open(os.path.join(desti, \"classifier.pkl\"), \"wb\"), protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.py として movieclassifier directory に保存する。\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# import re\n",
    "# import os\n",
    "# import pickle\n",
    " \n",
    "# current_dir = os.path.dirname(__file__)\n",
    "# stop = pickle.load(open(os.path.join(current_dir,\"movieclassifier\", \"pkl_objects\", \"stopwords.pkl\"), \"rb\"))\n",
    "# def tokenizer(text):\n",
    "#     text = re.sub(\"<[^>]*>\", \"\", text)\n",
    "#     emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text.lower())\n",
    "#     text = re.sub(\"[\\W]+\", \" \", text.lower()) + \" \".join(emoticons).replace(\"-\", \" \")\n",
    "#     tokenized = [w for w in text.split() if w not in stop]\n",
    "#     return tokenized\n",
    "\n",
    "# vect = HashingVectorizer(decode_error=\"igonore\", n_features=2**21, preprocessor=None, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import re\n",
    "import os \n",
    "from vectorizer import vect\n",
    "clf = pickle.load(open(os.path.join(\"movieclassifier\", \"pkl_objects\", \"classifier.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label = {0:\"negative\", 1:\"positive\"}\n",
    "example = [\"I love you forever\"]\n",
    "X = vect.transform(example)\n",
    "print(\"Prediction: {} \\nProbability {}\".format(label[clf.predict(X)[0]], np.max(clf.predict_proba(X)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "# if os.path.exists(\"reviews.sqlite\"):\n",
    "#     os.remove(\"reviews.sqlite\")\n",
    "conn = sqlite3.connect(\"reviews.sqlite\")\n",
    "conn.close()\n",
    "# c = conn.cursor()\n",
    "# c.execute(\"CREATE TABLE review_db (review TEXT, sentiment INTEGER, date TEXT)\")\n",
    "# example1 = \"I love the movie very much\"\n",
    "# c.execute(\"INSERT INTO review_db (review, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example1, 1))\n",
    "# example2 = \"I disliked the movie\"\n",
    "# c.execute(\"INSERT INTO review_db (review, sentiment, date) VALUES (?, ? ,DATETIME('now'))\", (example2, 0))\n",
    "# conn.commit()\n",
    "# conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect(\"reviews.sqlite\")\n",
    "# c = conn.cursor()\n",
    "# c.execute(\"SELECT * FROM review_db WHERE date BETWEEN '2017-01-01 00:00:00' AND DATETIME('now')\")\n",
    "# redults = c.fetchall()\n",
    "# conn.close()\n",
    "# print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
