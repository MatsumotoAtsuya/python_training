{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation McCulloch-Pitts neuron\n",
    "#multilayer feedforward neural network\n",
    "#hidden layer, bias unit, one-versus-all, \n",
    "import os \n",
    "import struct\n",
    "import numpy as np\n",
    "# def load_mnist(path, kind=\"train\"):\n",
    "#     labels_path = os.path.join(path, \"{}-labels.idx1-ubyte\".format(kind))\n",
    "#     images_path = os.path.join(path, \"{}-images.idx3-ubyte\".format(kind))\n",
    "#     #read files\n",
    "#     with open(labels_path, \"rb\") as lbpath:\n",
    "#         magic, n = struct.unpack(\">II\", lbpath.read(8))\n",
    "#     #magic number, number of items\n",
    "#         labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "    \n",
    "#     with open(images_path, \"rb\") as imgpath:\n",
    "#         magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "#         images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "#         images = ((images / 255.0) - 0.5) * 2\n",
    "    \n",
    "#     return images, labels\n",
    "\n",
    "# #magic, n  = struct.unpack(\">II\", lbpath.read(8))\n",
    "# #magic number >> file protocol \n",
    "# # >II >> edian and no sign integralã€€(read from head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = load_mnist(\"\", kind=\"train\")\n",
    "# print(\"Rows: {}, Columns: {}\".format(X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test, y_test = load_mnist(\"\", kind=\"t10k\")\n",
    "# print(\"Rows: {}, Columns: {}\".format(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "# ax = ax.flatten()\n",
    "# for i in range(10):\n",
    "#     img = X_train[y_train == i][0].reshape(28, 28)\n",
    "#     ax[i].imshow(img, cmap=\"Greys\")\n",
    "# ax[0].set_xticks([])\n",
    "# ax[0].set_yticks([])\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "# ax = ax.flatten()\n",
    "# for i in range(25):\n",
    "#     #print(X_train[y_train == 7][i])\n",
    "#     img = X_train[y_train == 7][i].reshape(28,28)\n",
    "#     ax[i].imshow(img, cmap=\"Greys\")\n",
    "\n",
    "# ax[0].set_xticks([])\n",
    "# ax[0].set_yticks([])\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.savez_compressed(\"mnist_scaled.npz\", X_train=X_train,\n",
    "#                    y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.load(\"mnist_scaled.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist[\"X_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = [mnist[f] for f in mnist.files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from neuralnet import NeuralNetMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP(object):\n",
    "    #n_hidden >> number of hidden layers\n",
    "    #l2 >> l2 regulariaion paramator\n",
    "    #shuffle >> shuffle every epoch or not\n",
    "    #eval >> evalation dictionary\n",
    "    #epochs >> number of training\n",
    "    #eta >> 7th Greek alphabet\n",
    "    def __init__(self, n_hidden=30, l2=0.0, epochs=100, eta=0.01,\n",
    "                shuffle=True, minibatch_size=1, seed=None):\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "    def _onehot(self, y, n_classes):\n",
    "        onehot = np.zeros((n_classes, y.shape[0]))\n",
    "        for ind, val in enumerate(y.astype(int)):\n",
    "            onehot[val, ind] = 1\n",
    "        return onehot.T\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n",
    "    #np.clip(val, min, max)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        z_h = np.dot(X, self.w_h) + self.b_h\n",
    "        #total hidden_layer\n",
    "        a_h = self._sigmoid(z_h)\n",
    "        #activate hidden layer\n",
    "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
    "        a_out = self._sigmoid(z_out)\n",
    "        return z_h, a_h, z_out, a_out\n",
    "    \n",
    "    def _compute_cost(self, y_enc, output):\n",
    "        #y_enc >> encoded y by onehot encoder\n",
    "        #output activeted outlayer\n",
    "        L2_term = (self.l2 * (np.sum(self.w_h ** 2.0) +\n",
    "                             np.sum(self.w_out ** 2.0)))\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 -term2) + L2_term\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #prediction\n",
    "        z_h, a_h, z_out, a_out = self._forward(X)\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        #sample features for validation << X_valid, y_valid\n",
    "        n_output = np.unique(y_train).shape[0]\n",
    "        n_features = X_train.shape[1]\n",
    "    #initialize weights\n",
    "        self.b_h = np.zeros(self.n_hidden)\n",
    "        self.w_h = np.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden))\n",
    "        self.b_out = np.zeros(n_output)\n",
    "        self.w_out = np.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output))\n",
    "        epoch_strlen = len(str(self.epochs))\n",
    "        self.eval_ = {\"cost\": [], \"train_accuracy\": [], \"valid_accuracy\": []}\n",
    "        #dictionary for evaluation\n",
    "        y_train_enc = self._onehot(y_train, n_output)\n",
    "        #training in epoch number times\n",
    "        for i in range(self.epochs):\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            #indices >> plural index (not indexes)\n",
    "            if self.shuffle:\n",
    "                self.random.shuffle(indices)\n",
    "            for start_ind in range(0,\n",
    "                                  indices.shape[0] - self.minibatch_size+1,\n",
    "                                  self.minibatch_size):\n",
    "                batch_ind = indices[start_ind: start_ind + self.minibatch_size]\n",
    "                z_h, a_h, z_out, a_out = self._forward(X_train[batch_ind])\n",
    "                #backpropagation\n",
    "                sigma_out = a_out - y_train_enc[batch_ind]\n",
    "                sigmoid_derivation_h = a_h * (1.0 - a_h)\n",
    "                sigma_h = (np.dot(sigma_out, self.w_out.T) *\n",
    "                          sigmoid_derivation_h)\n",
    "                grad_w_h = np.dot(X_train[batch_ind].T, sigma_h)\n",
    "                grad_b_h = np.sum(sigma_h, axis=0)\n",
    "                grad_w_out = np.dot(a_h.T, sigma_out)\n",
    "                grad_b_out = np.sum(sigma_out, axis=0)\n",
    "                #regularization and update weights\n",
    "                delta_w_h = (grad_w_h + self.l2*self.w_h)\n",
    "                delta_b_h = grad_b_h\n",
    "                self.w_h -= self.eta * delta_w_h\n",
    "                self.b_h -= self.eta * delta_b_h\n",
    "                \n",
    "                delta_w_out = (grad_w_out + self.l2*self.w_out)\n",
    "                delta_b_out = grad_b_out\n",
    "                self.w_out -= self.eta * delta_w_out\n",
    "                self.b_out -= self.eta * delta_b_out\n",
    "            #evaluation in every iteration\n",
    "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
    "            cost = self._compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "            \n",
    "            y_train_pred = self.predict(X_train)\n",
    "            y_valid_pred = self.predict(X_valid)\n",
    "            \n",
    "            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) / X_train.shape[0])\n",
    "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) / X_valid.shape[0])\n",
    "            import sys \n",
    "            sys.stderr.write(\"  {:strlen}/{} | Cost: {:2f} | Train/Valid acc. {:2f} / {:2f}\".format(i+1,\n",
    "                                                            self.epochs, cost, train_acc*100, valid_acc*100))\n",
    "            sys.stderr.flush()\n",
    "            \n",
    "            self.eval_[\"cost\"].append(cost)\n",
    "            self.eval_[\"train_accuracy\"].append(train_acc)\n",
    "            self.eval_[\"valid_accuracy\"].append(valid_acc)\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "nn = NeuralNetMLP(n_hidden=100, l2=0.01, epochs=n_epochs,\n",
    "                      eta=0.0005, minibatch_size=100, shuffle=True, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(X_train=X_train[:55000], y_train=y_train[:55000],\n",
    "      X_valid=X_train[55000:], y_valid=y_train[55000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(nn.epochs), nn.eval_[\"cost\"])\n",
    "plt.xlabel(\"Cost\")\n",
    "plt.ylabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(nn.epochs), nn.eval_[\"train_accuracy\"], label=\"training\")\n",
    "plt.plot(range(nn.epochs), nn.eval_[\"valid_accuracy\"], label=\"validation\",\n",
    "        linestyle=\"--\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = nn.predict(X_test)\n",
    "#score of model accuracy in test data set\n",
    "acc = (np.sum(y_test == y_test_pred).astype(np.float) / X_test.shape[0])\n",
    "print(\"training accuracy: {:2f}\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miscal_img = X_test[y_test != y_test_pred][:25]\n",
    "correct_lab = y_test[y_test != y_test_pred][:25]\n",
    "miscal_lab = y_test_pred[y_test != y_test_pred][:25]\n",
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = miscal_img[i].reshape(28,28)\n",
    "    ax[i].imshow(img, cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    ax[i].set_title(\"{:d} ) t:{:d} p:{:d}\".format(i+1, correct_lab[i], miscal_lab[i]))\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
